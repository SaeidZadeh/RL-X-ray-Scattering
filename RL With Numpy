import numpy as np

# Define the environment class
class Environment:
    def __init__(self, reflectivity_values, time_measurement, parameters, X):
        self.reflectivity_values = reflectivity_values
        self.time_measurement = time_measurement
        self.parameters = parameters
        self.X = X

    def reset(self):
        # Reset the environment state
        self.state = {
            'positions': self.X,
            'reflectivity': self.reflectivity_values[:, self.X],
            'time': self.time_measurement[:, self.X]
        }
        return self.state

    def step(self, action):
        # Update the environment state based on the selected action
        new_position = action

        # Measure reflectivity and time at the new position
        new_reflectivity = self.reflectivity_values[:, new_position]
        new_time = self.time_measurement[:, new_position]
        # Read next q-point time measurement from a file
        '''
        file = open("example.txt", "r")
        # Move the file pointer to the desired position
        file.seek(10)  # Seek to the 10th byte in the file
        # Read the desired data from the file
        file_data = file.read(5)  # Read 5 characters from the current position
        # Close the file
        file.close()
        # Validate if the input is numeric
        if user_input.isnumeric():
        # Convert the input to integer or float if required
            numeric_input = float(user_input)
        new_time = file_data
        '''
        # Read next q-point time measurement from a user input
        '''
        # Prompt the user for input
        user_data = input("Enter a value: ")
                # Validate if the input is numeric
        if user_input.isnumeric():
        # Convert the input to integer or float if required
            numeric_input = float(user_input)
        new_time = user_data
        '''
        #This can be also applied to new_reflectivity


        # Update the state
        self.state['positions'] = np.append(self.state['positions'], new_position)
        self.state['reflectivity'] = np.append(self.state['reflectivity'], new_reflectivity)
        self.state['time'] = np.append(self.state['time'], new_time)

        # Calculate the reward based on prediction accuracy
        predicted_parameters = agent.predict(self.state['reflectivity'], self.state['time'])
        reward = -np.mean(np.abs(predicted_parameters - self.parameters))

        # Check if all positions have been selected
        done = len(self.state['positions']) == len(self.reflectivity_values[0])

        return self.state, reward, done

# Define the DQN agent
class DQNAgent:
    def __init__(self, state_size, action_size, hidden_size=64, learning_rate=0.001, gamma=0.99, epsilon_decay=0.999,
                 memory_size=2000, batch_size=32):
        self.state_size = state_size
        self.action_size = action_size
        self.hidden_size = hidden_size
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = 1.0
        self.epsilon_decay = epsilon_decay
        self.memory = []
        self.memory_size = memory_size
        self.batch_size = batch_size
        self.weights = self.initialize_weights()

    def initialize_weights(self):
        # Initialize the neural network weights
        weights = {
            'W1': np.random.randn(self.hidden_size, self.state_size) * 0.01,
            'b1': np.zeros((self.hidden_size, 1)),
            'W2': np.random.randn(self.hidden_size, self.hidden_size) * 0.01,
            'b2': np.zeros((self.hidden_size, 1)),
            'W3': np.random.randn(self.action_size, self.hidden_size) * 0.01,
            'b3': np.zeros((self.action_size, 1))
        }
        return weights

    def forward_propagation(self, state):
        # Perform forward propagation to compute Q-values
        Z1 = np.dot(self.weights['W1'], state) + self.weights['b1']
        A1 = np.maximum(0, Z1)
        Z2 = np.dot(self.weights['W2'], A1) + self.weights['b2']
        A2 = np.maximum(0, Z2)
        Z3 = np.dot(self.weights['W3'], A2) + self.weights['b3']
        return Z3

    def act(self, state):
        # Epsilon-greedy action selection
        if np.random.rand() <= self.epsilon:
            return np.random.choice(self.action_size)
        q_values = self.forward_propagation(state)
        return np.argmax(q_values)

    def remember(self, state, action, reward, next_state, done):
        # Store the experience in the replay memory
        self.memory.append((state, action, reward, next_state, done))
        if len(self.memory) > self.memory_size:
            self.memory.pop(0)

    def replay(self):
        # Perform replay and update the neural network weights
        if len(self.memory) < self.batch_size:
            return
        minibatch = random.sample(self.memory, self.batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target += self.gamma * np.amax(self.forward_propagation(next_state))
            q_values = self.forward_propagation(state)
            q_values[action] = target
            self.back_propagation(state, q_values)

        if self.epsilon > 0.01:
            self.epsilon *= self.epsilon_decay

    def back_propagation(self, state, q_values):
        # Perform backpropagation to update the neural network weights
        dZ3 = q_values
        dW3 = np.dot(dZ3, self.weights['W3'])
        db3 = np.sum(dZ3, axis=1, keepdims=True)
        dA2 = np.dot(self.weights['W3'].T, dZ3)
        dZ2 = dA2 * (self.weights['b2'] > 0)
        dW2 = np.dot(dZ2, self.weights['W2'])
        db2 = np.sum(dZ2, axis=1, keepdims=True)
        dA1 = np.dot(self.weights['W2'].T, dZ2)
        dZ1 = dA1 * (self.weights['b1'] > 0)
        dW1 = np.dot(dZ1, self.weights['W1'])
        db1 = np.sum(dZ1, axis=1, keepdims=True)

        self.weights['W1'] -= self.learning_rate * dW1
        self.weights['b1'] -= self.learning_rate * db1
        self.weights['W2'] -= self.learning_rate * dW2
        self.weights['b2'] -= self.learning_rate * db2
        self.weights['W3'] -= self.learning_rate * dW3
        self.weights['b3'] -= self.learning_rate * db3

    def predict(self, reflectivity, time):
        # Predict the parameters based on reflectivity and time measurements
        state = np.concatenate((reflectivity, time), axis=0)
        return self.forward_propagation(state)

# Set the parameters
N = 10000
num_positions = 1024
num_parameters = 4
X = np.random.choice(num_positions, N, replace=False)
reflectivity_values = np.random.rand(num_parameters, num_positions)
time_measurement = np.random.rand(num_parameters, num_positions)
parameters = np.random.rand(num_parameters)

# Create the environment
env = Environment(reflectivity_values, time_measurement, parameters, X)

# Define the state and action sizes
state_size = num_parameters * len(X)
action_size = num_positions - len(X)

# Create the agent
agent = DQNAgent(state_size, action_size)

# Train the agent
episodes = 1000
for episode in range(episodes):
    state = env.reset()
    done = False
    while not done:
        action = agent.act(state)
        next_state, reward, done = env.step(action)
        agent.remember(state, action, reward, next_state, done)
        state = next_state
    agent.replay()

# Save the weights
np.savez('weights.npz', **agent.weights)

# Load the weights
weights = np.load('weights.npz')

# Use the trained agent to make a measurement on a new data point
new_X = random.sample(range(1024), 100)  # New subset X with 100 q-points
new_reflectivity = reflectivity_values[:, new_X]
new_time = time_measurement[:, new_X]
state = {
    'positions': new_X,
    'reflectivity': new_reflectivity,
    'time': new_time
}
action = agent.act(state)
next_state, ground_truth_parameters_y, time_measurement_y = env.step(action)
predicted_parameters = agent.forward(state)[0]

print("Measurement on New Data Point:")
print(f"Selected q-point: {action}")
print(f"Measured time: {time_measurement_y}")
print(f"Predicted parameters: {predicted_parameters}")

predicted_parameters = agent.forward(next_state)[0]
print(f"Predicted Parameters on the next state: {predicted_parameters}")
